{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de85e6c7",
   "metadata": {},
   "source": [
    "# Lab 6\n",
    "\n",
    "## Submission checklist¶\n",
    "1. Edit this file, add, commit, and push the changes to GitHub. Do not add other files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ce73f",
   "metadata": {},
   "source": [
    "## Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ada6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "!conda install --quiet --yes -c conda-forge pbzip2 zstd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb80f2",
   "metadata": {},
   "source": [
    "## Overview and references\n",
    "\n",
    "You will write a small bit of Python code to generate random binary and FASTA files. Your\n",
    "random binary files will contain a specified percentage of zeroes and ones, and your random\n",
    "FASTA files will contain either DNA or protein sequences. Using this code, you will generate a\n",
    "set of random data which you will then compress using gzip and bzip2. You will generate a table\n",
    "comparing the original file size, compressed file size, and run time for each algorithm on each\n",
    "sequence.\n",
    "\n",
    "### Information theory\n",
    "https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "### Compression algorithms\n",
    "https://en.wikipedia.org/wiki/Gzip  \n",
    "https://en.wikipedia.org/wiki/Bzip2  \n",
    "https://en.wikipedia.org/wiki/Zstandard  \n",
    "\n",
    "### Numpy\n",
    "https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html  \n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.packbits.html\n",
    "\n",
    "### Single-letter amino acid code\n",
    "https://en.wikipedia.org/wiki/Proteinogenic_amino_acid\n",
    "\n",
    "## Background\n",
    "You’re working at a biotech company that generates 1000 terabytes of data every day. In a\n",
    "meeting, your boss mentions that it costs the company \\\\$50 per terabyte of hard disk space, and\n",
    "so every 1% reduction in data that must be stored translates into a \\\\$500 savings per day. Your\n",
    "team will get a bonus this year equal to the amount of savings you’re able to generate by\n",
    "compressing the company’s data.\n",
    "\n",
    "Incentivized by this bonus, your team gets to work determining which compression algorithm\n",
    "will generate the most savings. The algorithm you choose must either be quick enough to\n",
    "compress 1000 terabytes a day, or efficient enough that even if all the data isn’t compressed,\n",
    "the savings are maximized by the data that you have time to compress.\n",
    "\n",
    "## Part 1 - Simulating the data\n",
    "Fortunately, you know that most of the data you’ll be compressing will be nucleic acid\n",
    "sequences. Some of it, however, will be binary files, and some will be protein sequences. Start\n",
    "by writing some code to simulate files containing random DNA, protein, and binary data.\n",
    "\n",
    "Using `np.random.choice` , generate **100 megabytes** (8 bits/byte $\\times$ 1024 bytes/kilobyte $\\times$ 1024\n",
    "kilobytes/megabyte $\\times$ 100) of random data containing 100%, 90%, 80%, 70%, 60%, and 50%\n",
    "zeros. Be sure to call `np.packbits` on your data before writing it to a file. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb6fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "example = np.random.choice([0, 1], size=1024, replace=True, p=[0.5, 0.5])\n",
    "#example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09cf6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.packbits(example)\n",
    "#example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e2069f",
   "metadata": {},
   "source": [
    "Then write this data to a file in your home directory, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95cfd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example\", \"wb\") as f:\n",
    "    f.write(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f71c1",
   "metadata": {},
   "source": [
    "You may notice that your kernel crashes if you try to generate and write too much data at once.\n",
    "The DataHub containers only allocate 1GB of memory for JupyterHub. You will need to be\n",
    "creative when generating and writing large amounts of data. Consider using `open(filename, “a”)` or `open(filename, “ab”)` to append to an existing file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47a0c3",
   "metadata": {},
   "source": [
    "### *Implement the function below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85c8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_random_binary(prob_zero, length, output_filename):\n",
    "    \n",
    "    remaining_length = length\n",
    "    max_write = 8*1024*1024 #Don't write more than a megabyte at a time\n",
    "    f = open(output_filename, \"wb\") #Delete the file if it exists\n",
    "    f.close()\n",
    "    f = open(output_filename, \"ab\")\n",
    "    \n",
    "    while remaining_length > 0:\n",
    "\n",
    "        write_length = min(remaining_length, max_write)\n",
    "        binary_array = np.random.choice([0,1], size=write_length, replace=True, p=[prob_zero, 1-prob_zero])\n",
    "        binary_array = np.packbits(binary_array)\n",
    "        f.write(binary_array)\n",
    "        remaining_length -= write_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cca9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [1, .9, .8, .7, .6, .5]\n",
    "file_name_suffix = \"_binary_array\"\n",
    "length = 8 * 1024 * 1024 * 100\n",
    "\n",
    "for prob in probs:\n",
    "    output_file_name = str(prob)+file_name_suffix\n",
    "    write_random_binary(prob, length, output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1dad1",
   "metadata": {},
   "source": [
    "Next, generate DNA and protein sequences **100 million** letters long and write those to your\n",
    "directory. The probability distribution over nucleotides/amino acids should be uniform. To write\n",
    "strings generated in Numpy to a file, you’ll have to use a slightly different command, like this:\n",
    "```python\n",
    "with open(“nt_seq.fa”, “w”) as f:\n",
    "    f.write(“”.join(my_nt_seq))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db8de6",
   "metadata": {},
   "source": [
    "### *Implement the function below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443e309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_random_dna(probs, length, output_filename):\n",
    "    \n",
    "    remaining_length = length\n",
    "    max_write = 1024*1024 \n",
    "    f = open(output_filename, \"w\") #Delete the file if it exists\n",
    "    f.close()\n",
    "    f = open(output_filename, \"a\")\n",
    "    alphabet = [\"A\", \"T\", \"G\", \"C\"]\n",
    "    \n",
    "    while remaining_length > 0:\n",
    "            \n",
    "        write_length = min(remaining_length, max_write)\n",
    "        rand_array = np.random.choice(alphabet, size=write_length, replace=True, p=probs)\n",
    "        f.write(\"\".join(rand_array))\n",
    "        remaining_length -= write_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bd969e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_random_dna([.25 for i in range(4)], 100000000, \"dna_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f719d8",
   "metadata": {},
   "source": [
    "### *Implement the function below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57f0ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_random_protein(probs, length, output_filename):\n",
    "    \n",
    "    remaining_length = length\n",
    "    max_write = 1024*1024 \n",
    "    f = open(output_filename, \"w\") #Delete the file if it exists\n",
    "    f.close()\n",
    "    f = open(output_filename, \"a\")\n",
    "    \n",
    "    while remaining_length > 0:\n",
    "            \n",
    "        alphabet = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "        write_length = min(remaining_length, max_write)\n",
    "        rand_array = np.random.choice(alphabet, size=write_length, replace=True, p=probs)\n",
    "        f.write(\"\".join(rand_array))\n",
    "        remaining_length -= write_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e25f51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_random_protein([.05 for i in range(20)], 100000000, \"protein_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10414500",
   "metadata": {},
   "source": [
    "## Part 2 - Compressing the data\n",
    "\n",
    "The time command on Linux will run whatever commands follow and report how long it took. Gzip and Bzip2 will silently compress your file and produce\n",
    "a new file with the extension .gz or .bz2, respectively. The `-k` option tells them not to delete the\n",
    "original file once the compression has completed. `-f` overwrites the output file if it already existed.\n",
    "\n",
    "*On each of the files you generated\n",
    "above*, run gzip, bzip, pbzip2, and zstd as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "184cd208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% Binary\n",
      "\n",
      "real\t0m1.021s\n",
      "user\t0m0.765s\n",
      "sys\t0m0.200s\n",
      "\n",
      "real\t0m1.591s\n",
      "user\t0m1.440s\n",
      "sys\t0m0.125s\n",
      "\n",
      "real\t0m0.520s\n",
      "user\t0m2.049s\n",
      "sys\t0m0.218s\n",
      "1_binary_array       :  0.00%   (104857600 =>   3307 bytes, 1_binary_array.zst) \n",
      "\n",
      "real\t0m0.164s\n",
      "user\t0m0.140s\n",
      "sys\t0m0.108s\n",
      "90% Binary\n",
      "\n",
      "real\t0m25.973s\n",
      "user\t0m25.378s\n",
      "sys\t0m0.450s\n",
      "\n",
      "real\t0m20.118s\n",
      "user\t0m19.563s\n",
      "sys\t0m0.232s\n",
      "\n",
      "real\t0m7.398s\n",
      "user\t0m29.674s\n",
      "sys\t0m1.287s\n",
      "0.9_binary_array     : 54.71%   (104857600 => 57371503 bytes, 0.9_binary_array.zst) \n",
      "\n",
      "real\t0m4.394s\n",
      "user\t0m3.619s\n",
      "sys\t0m0.253s\n",
      "80% Binary\n",
      "\n",
      "real\t0m20.598s\n",
      "user\t0m19.239s\n",
      "sys\t0m0.681s\n",
      "\n",
      "real\t0m29.030s\n",
      "user\t0m28.253s\n",
      "sys\t0m0.305s\n",
      "\n",
      "real\t0m8.710s\n",
      "user\t0m38.894s\n",
      "sys\t0m1.370s\n",
      "0.8_binary_array     : 73.91%   (104857600 => 77499767 bytes, 0.8_binary_array.zst) \n",
      "\n",
      "real\t0m2.819s\n",
      "user\t0m2.425s\n",
      "sys\t0m0.257s\n",
      "70% Binary\n",
      "\n",
      "real\t0m9.322s\n",
      "user\t0m8.269s\n",
      "sys\t0m0.570s\n",
      "\n",
      "real\t0m34.863s\n",
      "user\t0m34.496s\n",
      "sys\t0m0.261s\n",
      "\n",
      "real\t0m7.676s\n",
      "user\t0m46.304s\n",
      "sys\t0m1.354s\n",
      "0.7_binary_array     : 88.65%   (104857600 => 92953895 bytes, 0.7_binary_array.zst) \n",
      "\n",
      "real\t0m0.972s\n",
      "user\t0m0.486s\n",
      "sys\t0m0.263s\n",
      "60% Binary\n",
      "\n",
      "real\t0m6.308s\n",
      "user\t0m5.251s\n",
      "sys\t0m0.537s\n",
      "\n",
      "real\t0m43.471s\n",
      "user\t0m42.974s\n",
      "sys\t0m0.347s\n",
      "\n",
      "real\t0m9.407s\n",
      "user\t0m57.964s\n",
      "sys\t0m1.523s\n",
      "0.6_binary_array     : 97.46%   (104857600 => 102197306 bytes, 0.6_binary_array.zst) \n",
      "\n",
      "real\t0m0.999s\n",
      "user\t0m0.550s\n",
      "sys\t0m0.292s\n",
      "50% Binary\n",
      "\n",
      "real\t0m5.372s\n",
      "user\t0m4.259s\n",
      "sys\t0m0.503s\n",
      "\n",
      "real\t0m49.156s\n",
      "user\t0m48.307s\n",
      "sys\t0m0.418s\n",
      "\n",
      "real\t0m12.321s\n",
      "user\t1m2.452s\n",
      "sys\t0m1.639s\n",
      "0.5_binary_array     :100.00%   (104857600 => 104860014 bytes, 0.5_binary_array.zst) \n",
      "\n",
      "real\t0m1.070s\n",
      "user\t0m0.505s\n",
      "sys\t0m0.406s\n",
      "DNA\n",
      "\n",
      "real\t0m15.677s\n",
      "user\t0m14.813s\n",
      "sys\t0m0.681s\n",
      "\n",
      "real\t0m13.952s\n",
      "user\t0m13.678s\n",
      "sys\t0m0.167s\n",
      "\n",
      "real\t0m3.531s\n",
      "user\t0m20.861s\n",
      "sys\t0m1.154s\n",
      "dna_array            : 31.20%   (100000000 => 31196884 bytes, dna_array.zst)   \n",
      "\n",
      "real\t0m2.940s\n",
      "user\t0m2.797s\n",
      "sys\t0m0.302s\n",
      "Protein\n",
      "\n",
      "real\t0m6.325s\n",
      "user\t0m5.489s\n",
      "sys\t0m0.490s\n",
      "\n",
      "real\t0m20.660s\n",
      "user\t0m20.100s\n",
      "sys\t0m0.285s\n",
      "\n",
      "real\t0m4.946s\n",
      "user\t0m30.221s\n",
      "sys\t0m1.288s\n",
      "protein_array        : 55.65%   (100000000 => 55650704 bytes, protein_array.zst) \n",
      "\n",
      "real\t0m2.653s\n",
      "user\t0m2.354s\n",
      "sys\t0m0.252s\n"
     ]
    }
   ],
   "source": [
    "print(\"100% Binary\")\n",
    "!time gzip -k -f 1_binary_array\n",
    "!time bzip2 -k -f 1_binary_array\n",
    "!time pbzip2 -k -f 1_binary_array\n",
    "!time zstd -k -f 1_binary_array\n",
    "\n",
    "print(\"90% Binary\")\n",
    "!time gzip -k -f 0.9_binary_array\n",
    "!time bzip2 -k -f 0.9_binary_array\n",
    "!time pbzip2 -k -f 0.9_binary_array\n",
    "!time zstd -k -f 0.9_binary_array\n",
    "\n",
    "print(\"80% Binary\")\n",
    "!time gzip -k -f 0.8_binary_array\n",
    "!time bzip2 -k -f 0.8_binary_array\n",
    "!time pbzip2 -k -f 0.8_binary_array\n",
    "!time zstd -k -f 0.8_binary_array\n",
    "\n",
    "print(\"70% Binary\")\n",
    "!time gzip -k -f 0.7_binary_array\n",
    "!time bzip2 -k -f 0.7_binary_array\n",
    "!time pbzip2 -k -f 0.7_binary_array\n",
    "!time zstd -k -f 0.7_binary_array\n",
    "\n",
    "print(\"60% Binary\")\n",
    "!time gzip -k -f 0.6_binary_array\n",
    "!time bzip2 -k -f 0.6_binary_array\n",
    "!time pbzip2 -k -f 0.6_binary_array\n",
    "!time zstd -k -f 0.6_binary_array\n",
    "\n",
    "print(\"50% Binary\")\n",
    "!time gzip -k -f 0.5_binary_array\n",
    "!time bzip2 -k -f 0.5_binary_array\n",
    "!time pbzip2 -k -f 0.5_binary_array\n",
    "!time zstd -k -f 0.5_binary_array\n",
    "\n",
    "print(\"DNA\")\n",
    "!time gzip -k -f dna_array\n",
    "!time bzip2 -k -f dna_array\n",
    "!time pbzip2 -k -f dna_array\n",
    "!time zstd -k -f dna_array\n",
    "\n",
    "print(\"Protein\")\n",
    "!time gzip -k -f protein_array\n",
    "!time bzip2 -k -f protein_array\n",
    "!time pbzip2 -k -f protein_array\n",
    "!time zstd -k -f protein_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1eb3717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100M 0.5_binary_array\t   276K 1.69_entropy_dna.zst\r\n",
      "101M 0.5_binary_array.bz2  980K 1.86_entropy_dna\r\n",
      "101M 0.5_binary_array.gz   264K 1.86_entropy_dna.bz2\r\n",
      "101M 0.5_binary_array.zst  284K 1.86_entropy_dna.gz\r\n",
      "100M 0.6_binary_array\t   292K 1.86_entropy_dna.zst\r\n",
      "101M 0.6_binary_array.bz2  100M 1_binary_array\r\n",
      " 98M 0.6_binary_array.gz   8.0K 1_binary_array.bz2\r\n",
      " 98M 0.6_binary_array.zst  100K 1_binary_array.gz\r\n",
      "100M 0.7_binary_array\t   4.0K 1_binary_array.zst\r\n",
      " 96M 0.7_binary_array.bz2   40K assignment.ipynb\r\n",
      " 90M 0.7_binary_array.gz    96M dna_array\r\n",
      " 89M 0.7_binary_array.zst   27M dna_array.bz2\r\n",
      "100M 0.8_binary_array\t    28M dna_array.gz\r\n",
      " 83M 0.8_binary_array.bz2   30M dna_array.zst\r\n",
      " 78M 0.8_binary_array.gz   4.0K example\r\n",
      " 74M 0.8_binary_array.zst   96M protein_array\r\n",
      "100M 0.9_binary_array\t    53M protein_array.bz2\r\n",
      " 59M 0.9_binary_array.bz2   58M protein_array.gz\r\n",
      " 57M 0.9_binary_array.gz    64M protein_array.zst\r\n",
      " 55M 0.9_binary_array.zst   32K random_dna_covid_length\r\n",
      "980K 1.36_entropy_dna\t   4.0K README.md\r\n",
      "204K 1.36_entropy_dna.bz2   44K sequence_rle.txt\r\n",
      "228K 1.36_entropy_dna.gz    32K sequence.txt\r\n",
      "248K 1.36_entropy_dna.zst  8.0K sequence.txt.bz2\r\n",
      "980K 1.69_entropy_dna\t    12K sequence.txt.gz\r\n",
      "252K 1.69_entropy_dna.bz2   12K sequence.txt.zst\r\n",
      "268K 1.69_entropy_dna.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -sh *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af90aa93",
   "metadata": {},
   "source": [
    "Keep track of the size of the input files, the size of the output files (tip: try `!ls -sh *`), and the time each command took to run in a table such as the one below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74a759",
   "metadata": {},
   "source": [
    "*Note: Numbers in table are from a different run that the one displayed in the terminal output above*\n",
    "\n",
    "|      File                 | Input File | Output File |    Time(s) |\n",
    "|---------------------------|------------|-------------|------------|\n",
    "|  100% zeros, gzip          |   100MiB    |    100KiB     |    0.755   |\n",
    "|  100% zeros, bzip2          |   100MiB    |    8.0KiB     |    1.036   |\n",
    "|  100% zeros, pbzip2          |   100MiB    |   3.2KiB     |    0.389   |\n",
    "|  100% zeros, zstd          |   100MiB    |    4.0KiB     |    0.082   |\n",
    "|  90% zeros, gzip          |   100MiB    |    57MiB     |    24.294   |\n",
    "|  90% zeros, bzip2          |   100MiB    |    59MiB     |    14.356   |\n",
    "|  90% zeros, pbzip2          |   100MiB    |    55MiB     |    3.535   |\n",
    "|  90% zeros, zstd          |   100MiB    |    55MiB     |    2.013   |\n",
    "|  80% zeros, gzip          |   100MiB    |    78MiB     |    17.728   |\n",
    "|  80% zeros, bzip2          |   100MiB    |    83KiB     |    16.907   |\n",
    "|  80% zeros, pbzip2          |   100MiB    |    74MiB     |    4.376   |\n",
    "|  80% zeros, zstd          |   100MiB    |    74KiB     |    1.886   |\n",
    "|  70% zeros, gzip          |   100MiB    |    90MiB     |    7.718   |\n",
    "|  70% zeros, bzip2          |   100MiB    |    96MiB     |    18.733   |\n",
    "|  70% zeros, pbzip2          |   100MiB    |    89MiB     |    4.524   |\n",
    "|  70% zeros, zstd          |   100MiB    |    89MiB     |    0.739   |\n",
    "|  60% zeros, gzip          |   100MiB    |    98MiB     |    5.640   |\n",
    "|  60% zeros, bzip2          |   100MiB    |    101MiB     |    20.900   |\n",
    "|  60% zeros, pbzip2          |   100MiB    |    97MiB     |    4.857   |\n",
    "|  60% zeros, zstd          |   100MiB    |    98MiB     |    0.823  |\n",
    "|  50% zeros, gzip          |   100MiB    |    101MiB     |    4.770   |\n",
    "|  50% zeros, bzip2          |   100MiB    |    101MiB     |    22.284   |\n",
    "|  50% zeros, pbzip2          |   100MiB    |    100MiB     |    6.060   |\n",
    "|  50% zeros, zstd          |   100MiB    |    101MiB     |    0.696  |\n",
    "|  DNA, gzip          |   96MiB    |    28MiB     |    14.234  |\n",
    "|  DNA, bzip2          |   96MiB    |    27MiB     |    11.686   |\n",
    "|  DNA, pbzip2          |   96MiB    |    30MiB     |    2.785   |\n",
    "|  DNA, zstd          |   96MiB    |    30MiB     |    1.174   |\n",
    "|  Protein, gzip          |   96MiB    |    58MiB     |    5.411   |\n",
    "|  Protein, bzip2          |   96MiB    |    53MiB     |    13.366   |\n",
    "|  Protein, pbzip2          |   96MiB    |    53MiB     |    2.876   |\n",
    "|  Protein, zstd          |   96MiB    |    54KiB     |    2.024  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac65e2",
   "metadata": {},
   "source": [
    "## Part 3 - Entropy Coding\n",
    "\n",
    "Use the function created earlier to generate 1 million nt DNA sequence with the following\n",
    "nucleotide distributions (A, T, G, C respectively): `[0.1, 0.25, 0.25, 0.4]`, `[0.1, 0.1, 0.1, 0.7]`, & `[0.5, 0.3, 0.1, 0.1]`. Then, compute the entropy of the generated sequence and compare that to the\n",
    "entropy of the true distribution. Run the compression algorithms from the previous section on\n",
    "each and see which tool gets closest to the ideal code in terms of bits/nucleotide.\n",
    "\n",
    "Record the true distribution, generated distribution, Shannon entropy, and bits/nucleotide for\n",
    "each compression tool in a table such as the one below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503f6d1",
   "metadata": {},
   "source": [
    "## gzip\n",
    " |     Sequence        |    true distribution      | actual distribution      | Shannon entropy | bits/nucleotide |\n",
    " |---------------------|---------------------------|--------------------------|-----------------|-----------------|\n",
    " |        ATGC         |    0.1,0.25,0.25,0.4      |   0.1001,0.2494,0.2504,0.4002      |      1.86       |       2.27      |\n",
    " |        ATGC         |    0.1,0.1,0.1,0.7      |   0.0997,0.0999,0.0996,0.7009      |      1.36       |       1.82      |\n",
    " |        ATGC         |    0.5,0.3,0.1,0.1      |   0.5000,0.2996,0.1002,0.1001      |      1.69       |       2.14      |\n",
    " \n",
    " ## bzip2\n",
    " |     Sequence        |    true distribution      | actual distribution      | Shannon entropy | bits/nucleotide |\n",
    " |---------------------|---------------------------|--------------------------|-----------------|-----------------|\n",
    " |        ATGC         |    0.1,0.25,0.25,0.4      |   0.1001,0.2494,0.2504,0.4002      |      1.86       |       2.11      |\n",
    " |        ATGC         |    0.1,0.1,0.1,0.7      |   0.0997,0.0999,0.0996,0.7009      |      1.36       |       1.63      |\n",
    " |        ATGC         |    0.5,0.3,0.1,0.1      |   0.5000,0.2996,0.1002,0.1001      |      1.69       |       2.02      |\n",
    " \n",
    " ## pbzip2\n",
    " |     Sequence        |    true distribution      | actual distribution      | Shannon entropy | bits/nucleotide |\n",
    " |---------------------|---------------------------|--------------------------|-----------------|-----------------|\n",
    " |        ATGC         |    0.1,0.25,0.25,0.4      |   0.1001,0.2494,0.2504,0.4002      |      1.86       |       2.39      |\n",
    " |        ATGC         |    0.1,0.1,0.1,0.7      |   0.0997,0.0999,0.0996,0.7009      |      1.36       |       2.03      |\n",
    " |        ATGC         |    0.5,0.3,0.1,0.1      |   0.5000,0.2996,0.1002,0.1001      |      1.69       |       2.26      |\n",
    " \n",
    " ## zstd\n",
    " |     Sequence        |    true distribution      | actual distribution      | Shannon entropy | bits/nucleotide |\n",
    " |---------------------|---------------------------|--------------------------|-----------------|-----------------|\n",
    " |        ATGC         |    0.1,0.25,0.25,0.4      |   0.1001,0.2494,0.2504,0.4002      |      1.86       |       2.34      |\n",
    " |        ATGC         |    0.1,0.1,0.1,0.7      |   0.0997,0.0999,0.0996,0.7009      |      1.36       |       1.98      |\n",
    " |        ATGC         |    0.5,0.3,0.1,0.1      |   0.5000,0.2996,0.1002,0.1001      |      1.69       |       2.21      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f823ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1000000\n",
    "prob_sets = [[0.1,0.25,0.25,0.4], [0.1,0.1,0.1,0.7], [0.5,0.3,0.1,0.1]]\n",
    "predicted_entropies = []\n",
    "\n",
    "def calc_entropy(prob_set): \n",
    "    entropy = 0\n",
    "    for prob in prob_set:\n",
    "        entropy -= prob*math.log(prob, 2)\n",
    "    entropy = round(entropy, 2)\n",
    "    return entropy\n",
    "\n",
    "for prob_set in prob_sets:\n",
    "    entropy = calc_entropy(prob_set)\n",
    "    predicted_entropies.append(entropy)\n",
    "    file_name = str(entropy) + \"_entropy_dna\"\n",
    "    #write_random_dna(prob_set, length, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get actual distributions and actual entropies\n",
    "actual_entropies = []\n",
    "\n",
    "for i in range(len(predicted_entropies)):\n",
    "    predicted_entropy = predicted_entropies[i]\n",
    "    prob_set = prob_sets[i]\n",
    "    \n",
    "    file_name = str(predicted_entropy) + \"_entropy_dna\"\n",
    "    f = open(file_name, \"r\")\n",
    "    seq = f.readlines()[0]\n",
    "    \n",
    "    counts = Counter(seq)\n",
    "    \n",
    "    chars = sum(counts.values())\n",
    "    \n",
    "    print(prob_set)\n",
    "    actual_dist = {k: round(v / chars,4) for k, v in counts.items()}\n",
    "    \n",
    "    actual_entropy = calc_entropy(list(actual_dist.values()))\n",
    "    actual_entropies.append(entropy)  \n",
    "    \n",
    "    print(\"Predicted Entropy\", predicted_entropy)\n",
    "    print(actual_dist)\n",
    "    print(\"Actual Entropy\", actual_entropy)\n",
    "    print()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "838d480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.86 Entropy\n",
      "\n",
      "real\t0m0.190s\n",
      "user\t0m0.164s\n",
      "sys\t0m0.010s\n",
      "\n",
      "real\t0m0.164s\n",
      "user\t0m0.141s\n",
      "sys\t0m0.008s\n",
      "\n",
      "real\t0m0.159s\n",
      "user\t0m0.152s\n",
      "sys\t0m0.012s\n",
      "1.86_entropy_dna     : 29.87%   (1000000 => 298737 bytes, 1.86_entropy_dna.zst) \n",
      "\n",
      "real\t0m0.030s\n",
      "user\t0m0.014s\n",
      "sys\t0m0.008s\n",
      "1.36 Entropy\n",
      "\n",
      "real\t0m0.141s\n",
      "user\t0m0.124s\n",
      "sys\t0m0.003s\n",
      "\n",
      "real\t0m0.142s\n",
      "user\t0m0.115s\n",
      "sys\t0m0.013s\n",
      "\n",
      "real\t0m0.132s\n",
      "user\t0m0.121s\n",
      "sys\t0m0.012s\n",
      "1.36_entropy_dna     : 25.39%   (1000000 => 253856 bytes, 1.36_entropy_dna.zst) \n",
      "\n",
      "real\t0m0.030s\n",
      "user\t0m0.015s\n",
      "sys\t0m0.006s\n",
      "1.69 Entropy\n",
      "\n",
      "real\t0m0.169s\n",
      "user\t0m0.150s\n",
      "sys\t0m0.004s\n",
      "\n",
      "real\t0m0.176s\n",
      "user\t0m0.150s\n",
      "sys\t0m0.012s\n",
      "\n",
      "real\t0m0.158s\n",
      "user\t0m0.150s\n",
      "sys\t0m0.013s\n",
      "1.69_entropy_dna     : 28.22%   (1000000 => 282173 bytes, 1.69_entropy_dna.zst) \n",
      "\n",
      "real\t0m0.030s\n",
      "user\t0m0.012s\n",
      "sys\t0m0.008s\n",
      "\n",
      "100M 0.5_binary_array\t   276K 1.69_entropy_dna.zst\n",
      "101M 0.5_binary_array.bz2  980K 1.86_entropy_dna\n",
      "101M 0.5_binary_array.gz   264K 1.86_entropy_dna.bz2\n",
      "101M 0.5_binary_array.zst  284K 1.86_entropy_dna.gz\n",
      "100M 0.6_binary_array\t   292K 1.86_entropy_dna.zst\n",
      "101M 0.6_binary_array.bz2  100M 1_binary_array\n",
      " 98M 0.6_binary_array.gz   8.0K 1_binary_array.bz2\n",
      " 98M 0.6_binary_array.zst  100K 1_binary_array.gz\n",
      "100M 0.7_binary_array\t   4.0K 1_binary_array.zst\n",
      " 96M 0.7_binary_array.bz2   40K assignment.ipynb\n",
      " 90M 0.7_binary_array.gz    96M dna_array\n",
      " 89M 0.7_binary_array.zst   27M dna_array.bz2\n",
      "100M 0.8_binary_array\t    28M dna_array.gz\n",
      " 83M 0.8_binary_array.bz2   30M dna_array.zst\n",
      " 78M 0.8_binary_array.gz   4.0K example\n",
      " 74M 0.8_binary_array.zst   96M protein_array\n",
      "100M 0.9_binary_array\t    53M protein_array.bz2\n",
      " 59M 0.9_binary_array.bz2   58M protein_array.gz\n",
      " 57M 0.9_binary_array.gz    54M protein_array.zst\n",
      " 55M 0.9_binary_array.zst   32K random_dna_covid_length\n",
      "980K 1.36_entropy_dna\t   4.0K README.md\n",
      "204K 1.36_entropy_dna.bz2   44K sequence_rle.txt\n",
      "228K 1.36_entropy_dna.gz    32K sequence.txt\n",
      "248K 1.36_entropy_dna.zst  8.0K sequence.txt.bz2\n",
      "980K 1.69_entropy_dna\t    12K sequence.txt.gz\n",
      "252K 1.69_entropy_dna.bz2   12K sequence.txt.zst\n",
      "268K 1.69_entropy_dna.gz\n"
     ]
    }
   ],
   "source": [
    "print(\"1.86 Entropy\")\n",
    "!time gzip -k -f 1.86_entropy_dna\n",
    "!time bzip2 -k -f 1.86_entropy_dna\n",
    "!time pbzip2 -k -f 1.86_entropy_dna\n",
    "!time zstd -k -f 1.86_entropy_dna\n",
    "\n",
    "print(\"1.36 Entropy\")\n",
    "!time gzip -k -f 1.36_entropy_dna\n",
    "!time bzip2 -k -f 1.36_entropy_dna\n",
    "!time pbzip2 -k -f 1.36_entropy_dna\n",
    "!time zstd -k -f 1.36_entropy_dna\n",
    "\n",
    "print(\"1.69 Entropy\")\n",
    "!time gzip -k -f 1.69_entropy_dna\n",
    "!time bzip2 -k -f 1.69_entropy_dna\n",
    "!time pbzip2 -k -f 1.69_entropy_dna\n",
    "!time zstd -k -f 1.69_entropy_dna\n",
    "\n",
    "print()\n",
    "\n",
    "!ls -sh *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e6c33",
   "metadata": {},
   "source": [
    "**Q1.** For each of the tools, research what compression schemes are being applied under the hood. Do they have any schemes in common?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c396288",
   "metadata": {},
   "source": [
    "Each of the algorithms uses Huffman encoding at some stage in the compression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e922f03f",
   "metadata": {},
   "source": [
    "**Q2.** What does it mean for a compression scheme to be lossless? Are these tools lossless compression schemes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a6895",
   "metadata": {},
   "source": [
    "Lossless compression means that we can perfectly reconstruct the original data from the compressed data. Each of the four algorithms used in this notebook are lossless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423611fc",
   "metadata": {},
   "source": [
    "**Q3.** Which algorithm achieves the best level of compression on each file type? Which\n",
    "algorithm is the fastest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2116f23d",
   "metadata": {},
   "source": [
    "| File Type | Best Compression | Fastest Compression |\n",
    "| -------- | ---------------- | ------------------- |\n",
    "| Binary | pbzip2 | zstd |\n",
    "| DNA | bzip2 | zstd |\n",
    "| Protein | bzip2 | zstd |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678203af",
   "metadata": {},
   "source": [
    "**Q4.** What is the difference between bzip2 and pbzip2? Do you expect one to be faster and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8c7da",
   "metadata": {},
   "source": [
    "pbzip2 has been modified from bzip2 to run on multiple processors, so I would expect it to run faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c423bac0",
   "metadata": {},
   "source": [
    "**Q5.** How does the level of compression change as the percentage of zeros increases? Why\n",
    "does this happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3ebb9",
   "metadata": {},
   "source": [
    "As the percentage of zeros increases, the level of compression increases as well. This is because with a higher proportion of zeros, we have a lower entropy, so it requires less information to encode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d91ef6",
   "metadata": {},
   "source": [
    "**Q6.** What is the minimum number of bits required to store a single DNA base? What is the minimum number of bits required to store an amino acid letter? (think about information theory for these questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6f8c8",
   "metadata": {},
   "source": [
    "The number of bits to store a dna base is Log_2_(4) = 2.00 and for an amino acid is is Log_2_(20) = 4.32. Note that to use binary encoding for an amino acid, we would have to round up to 5 bits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f52cfa4",
   "metadata": {},
   "source": [
    "**Q7.** In your tests, how many bits did gzip and bzip2 actually require to store your random\n",
    "DNA and protein sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c1ce6",
   "metadata": {},
   "source": [
    "*Bits per character*\n",
    "\n",
    "| Sequence | gzip | bzip2 | ideal |\n",
    "| -------- | ---- | ----- | ----- |\n",
    "| Uniform Random DNA | 2.35 | 2.27 | 2.00 |\n",
    "| Uniform Random Protein | 7.60 | 6.95 | 4.32 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede0165",
   "metadata": {},
   "source": [
    "**Q8.** Do gzip and bzip2 perform well on DNA and proteins?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d67fd",
   "metadata": {},
   "source": [
    "Not particularly. By simply doing a binary encoding we can guarantee 2 bits per nucleotide for a DNA and 5 bits per nucleotide for a protein. Both algorithms miss this mark for both sequence types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce1524f",
   "metadata": {},
   "source": [
    "**Q9.** Which algorithm achieves a compression limit closest to an ideal code over the non-\n",
    "uniform nucleotide distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f3cca",
   "metadata": {},
   "source": [
    "bzip2 achieves the compression closest to the limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb9463",
   "metadata": {},
   "source": [
    "## Part 4 - Compressing real data\n",
    "Now that you have a sense of how random data can be compressed, let’s have a look at some\n",
    "real biological data: the sequence of the SARS-COV-2 genome (`sequence.txt` included in the repository).\n",
    "\n",
    "**Q10.** A priori, do you expect to achieve better or worse compression here than random data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db823d23",
   "metadata": {},
   "source": [
    "I would expect to achieve better compression. I'm expecting that SARS-COV-2 genome will have some natural repetitive patterns/motifs in it that the algorithms will be able to exploit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e940080",
   "metadata": {},
   "source": [
    "Now, compress the sequence using gzip, bzip2, and zstd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21f47285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m0.021s\n",
      "user\t0m0.004s\n",
      "sys\t0m0.003s\n",
      "\n",
      "real\t0m0.020s\n",
      "user\t0m0.002s\n",
      "sys\t0m0.006s\n",
      "sequence.txt         : 31.67%   ( 29903 =>   9470 bytes, sequence.txt.zst)     \n",
      "\n",
      "real\t0m0.011s\n",
      "user\t0m0.003s\n",
      "sys\t0m0.002s\n"
     ]
    }
   ],
   "source": [
    "!time gzip -k -f sequence.txt\n",
    "!time bzip2 -k -f sequence.txt\n",
    "!time zstd -k -f sequence.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e629a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"sequence.txt\", \"r\")\n",
    "covid_length = len(f.readlines()[0])\n",
    "print(covid_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c00c74e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100M 0.5_binary_array\t   276K 1.69_entropy_dna.zst\r\n",
      "101M 0.5_binary_array.bz2  980K 1.86_entropy_dna\r\n",
      "101M 0.5_binary_array.gz   264K 1.86_entropy_dna.bz2\r\n",
      "101M 0.5_binary_array.zst  284K 1.86_entropy_dna.gz\r\n",
      "100M 0.6_binary_array\t   292K 1.86_entropy_dna.zst\r\n",
      "101M 0.6_binary_array.bz2  100M 1_binary_array\r\n",
      " 98M 0.6_binary_array.gz   8.0K 1_binary_array.bz2\r\n",
      " 98M 0.6_binary_array.zst  100K 1_binary_array.gz\r\n",
      "100M 0.7_binary_array\t   4.0K 1_binary_array.zst\r\n",
      " 96M 0.7_binary_array.bz2   44K assignment.ipynb\r\n",
      " 90M 0.7_binary_array.gz    96M dna_array\r\n",
      " 89M 0.7_binary_array.zst   27M dna_array.bz2\r\n",
      "100M 0.8_binary_array\t    28M dna_array.gz\r\n",
      " 83M 0.8_binary_array.bz2   30M dna_array.zst\r\n",
      " 78M 0.8_binary_array.gz   4.0K example\r\n",
      " 74M 0.8_binary_array.zst   96M protein_array\r\n",
      "100M 0.9_binary_array\t    53M protein_array.bz2\r\n",
      " 59M 0.9_binary_array.bz2   58M protein_array.gz\r\n",
      " 57M 0.9_binary_array.gz    54M protein_array.zst\r\n",
      " 55M 0.9_binary_array.zst   32K random_dna_covid_length\r\n",
      "980K 1.36_entropy_dna\t   4.0K README.md\r\n",
      "204K 1.36_entropy_dna.bz2   44K sequence_rle.txt\r\n",
      "228K 1.36_entropy_dna.gz    32K sequence.txt\r\n",
      "248K 1.36_entropy_dna.zst  8.0K sequence.txt.bz2\r\n",
      "980K 1.69_entropy_dna\t    12K sequence.txt.gz\r\n",
      "252K 1.69_entropy_dna.bz2   12K sequence.txt.zst\r\n",
      "268K 1.69_entropy_dna.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -sh *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e71a8",
   "metadata": {},
   "source": [
    "**Q11.** How does the compression ratio of this file compare to random data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd025b",
   "metadata": {},
   "source": [
    "| Sequence | gzip | bzip2 | zstd |\n",
    "| -------- | ---- | ----- | ----- |\n",
    "| Uniform Random DNA | 2.35 | 2.27 | 2.51 |\n",
    "| Sars-COV-2 genome | 2.37 | 2.18 | 2.53 |\n",
    "\n",
    "The compression ratio was smaller (though very close) on the random DNA for two out of the three algorithms (gzip and zstd). It is worth noting, however, that the smallest overall compression ratio came from bzip2 on the SARS-COV-2 genome. I will consider this to confirm my hypothesis. \n",
    "\n",
    "It is possible that the relatively small sequence size for the SARS-COV-2 genome (30K bases vs. 100M) leads to some inefficiencies in the file size due to the metadata required for decompression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7abc7",
   "metadata": {},
   "source": [
    "## Part 5 - Implementing Run Length Encoding\n",
    "Now let’s implement one of the compression schemes we learned about in lecture: run length\n",
    "encoding. Recall that run length encoding involves compressing stretches of identical characters\n",
    "into just a character and a count. For example, the string \"ACTTTGCC\" would become\n",
    "\"1A1C3T1G2C\". For this exercise, you must use native python (no external libraries).\n",
    "\n",
    "Write a function that converts a string into a run length encoded string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848835e5",
   "metadata": {},
   "source": [
    "### *Implement the function below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2a13fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_encoding(seq):\n",
    "    \n",
    "    current_char = seq[0]\n",
    "    current_count = 1\n",
    "    new_str = []\n",
    "    for i in range(1,len(seq)):\n",
    "        new_char = seq[i]\n",
    "        if new_char == current_char:\n",
    "            current_count += 1\n",
    "        else:\n",
    "            new_str.append(str(current_count))\n",
    "            new_str.append(str(current_char))\n",
    "            current_count = 1\n",
    "            current_char = new_char\n",
    "    \n",
    "    new_str.append(str(current_count))\n",
    "    new_str.append(str(current_char))\n",
    "    \n",
    "    return \"\".join(new_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94ebea",
   "metadata": {},
   "source": [
    "Then, generate a\n",
    "random DNA sequence (from a uniform distribution), which is the same length as the SARS-\n",
    "COV-2 genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "214b5efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dna_cov_length = write_random_dna([.25 for i in range(4)], covid_length, \"random_dna_covid_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e28b2",
   "metadata": {},
   "source": [
    "**Q12.** A priori, do you expect to achieve better or worse compression on than random data\n",
    "than the virus genome? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ced430",
   "metadata": {},
   "source": [
    "For the same reason as mentioned previously, I would expect to achieve better compression on the COVID data - I expect naturally occuring DNA to have more repetitive sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d8939",
   "metadata": {},
   "source": [
    "Run your RLE compression function on both sequences and calculate the compression ratio for\n",
    "each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a50b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45106\n",
      "43655\n"
     ]
    }
   ],
   "source": [
    "f = open(\"random_dna_covid_length\", \"r\")\n",
    "random_rle = run_length_encoding(f.readlines()[0])\n",
    "f.close()\n",
    "print(len(random_rle))\n",
    "\n",
    "f = open(\"sequence.txt\", \"r\")\n",
    "covid_rle = run_length_encoding(f.readlines()[0])\n",
    "f.close()\n",
    "print(len(covid_rle))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac143f6",
   "metadata": {},
   "source": [
    "**Q13.** Which sequence is compressed more by a run length encoding scheme?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d36115f",
   "metadata": {},
   "source": [
    "Both sequences end up with longer sequences with run length encoding than without, but the random DNA is slightly longer. (150% vs. 146% compression by string length). Though my hypothesis was technically correct, the margin was not as large as expected. This leads me to believe that the SARS-COV-2 DNA has very few repeats, which would be consistent with what we've learned about how efficient virus genomes are in storing information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e1c90b",
   "metadata": {},
   "source": [
    "## Part 6 - Estimating compression of 1000 terabytes\n",
    "Let’s make some assumptions about the contents of the data at your biotech company. Most of\n",
    "the data, say 80%, is re-sequencing of genomes and plasmids that are very similar to each\n",
    "other. Another 10% might be protein sequences, and the last 10% are binary microscope\n",
    "images which we’ll assume follow the worst-case scenario of being completely random.\n",
    "\n",
    "**Q14.** Given the benchmarking data you obtained in this lab, which algorithm do you propose to use\n",
    "for each type of data? Provide an estimate for the fraction of space you can save using your\n",
    "compression scheme. How much of a bonus do you anticipate receiving this year?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c31c18",
   "metadata": {},
   "source": [
    "*For the sake of this analysis, I will assume that we have one machine to run compression on, similar to the way it was done in this workbook*\n",
    "\n",
    "We will be able to achieve the best compression ratio on the DNA data, so we will start by focusing our computing time here. To encode ~100MB of DNA with our fastest algorithm, zstd, took 1.174 seconds. So to encode 800,000,000MB (800 TB) will take 8,000,000 * 1.174 = 939,200 seconds = 261 hours. We are overwhelmingly limited by time here, so will use this algorithm to compress as much as we can in a day. Assuming the same compression ratio that was achieved on the random data (which is likely conservative, as noted in previous sections) and 24 hours of computing time, we can compress 800TB * ((96-30)/96) * (24/261) = ~50TB. We will not use computing time on the protein data, which would not have been as efficient to compress, or the binary data, which would not have compressed at all. \n",
    "\n",
    "I expect my team's bonus this year to be 50TB * 50 dollars/TB/day * 365 days/year = $912,500."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
